permissions:
  - '*'
sessions:
    tailscale: 
      type: link

jobs:
  preprocessing:
    steps:
      - name: Preprocessing
        run: ./utils/steps-v3/preprocessing/preprocessing.sh 
      - name: Validating Target Resource
        run: ./utils/steps-v3/preprocessing/input_form_resource_wrapper.sh ${{ inputs.pwrl_host.resource.ip }}
      - name: Process Inputs
        run: | 
          ./utils/steps-v3/preprocessing/process_inputs_sh.sh 
          echo "export basepath=/me/session/${PW_USER}/${{ sessions.session }}"  >> resources/host/inputs.sh
      - name: Transfer Files to Controller
        run: ./utils/steps-v3/preprocessing/transfer_files_to_controller.sh
      - name: Controller Preprocessing
        run: ./utils/steps-v3/preprocessing/controller_preprocessing.sh
      - name: Initialize Cancel Script
        run: ./utils/steps-v3/preprocessing/initialize_cancel_script.sh

  controller_job:
    needs:
       - preprocessing
    steps:
      - name: Create Controller Session Script
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/controller/create_session_script.sh
      - name: Launch and Monitor Controller Job
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/controller/launch_and_monitor_job.sh
        cleanup: |
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" == "CONTROLLER" ]]; then
            ./kill.sh
          fi
      - name: Clean and Exit
        if: ${{ 'CONTROLLER' === inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/clean_and_exit.sh

  compute_job:
    needs:
       - preprocessing
    steps:
      - name: Create Compute Session Script
        if: ${{ 'CONTROLLER' !== inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/compute/create_session_script.sh
      - name:  Launch and Monitor Compute Job
        if: ${{ 'CONTROLLER' !== inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/compute/launch_and_monitor_job.sh
        cleanup: |
          if [[ "${{ inputs.pwrl_host.jobschedulertype }}" != "CONTROLLER" ]]; then
            ./kill.sh
          fi
      - name: Clean and Exit
        if: ${{ 'CONTROLLER' !== inputs.pwrl_host.jobschedulertype }}
        run: ./utils/steps-v3/clean_and_exit.sh


  create_session:
    needs:
       - preprocessing
    steps:
      - name: Set Session Name
        run: |
          session_name=$(pwd | rev | cut -d'/' -f1-2 | tr '/' '-' | rev)
          echo "session_name=${session_name}" | tee -a $OUTPUTS
      - name: Get URL
        env:
          sshcmd: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${{ inputs.pwrl_host.resource.ip }}
        run: |
          source resources/host/inputs.sh
          while true; do
            URL=$(${sshcmd} cat  ${resource_jobdir}/tailscale.up.log | grep -o 'https://login.tailscale.com/a/[a-zA-Z0-9]*')
            if [ -n "$URL" ]; then
              echo "URL found: $URL"
              break
            fi
            sleep 10
          done
          echo "URL=${URL}"  | tee -a $OUTPUTS
      - uses: parallelworks/update-session
        name: Start PW Session
        with:
          name: ${{ sessions.tailscale }}
          type: link
          url: "${{ needs.main.outputs.URL }}"

'on':
  execute:
    inputs:
      pwrl_host:
        type: group
        label: Server Host
        items:
          resource:
            type: compute-clusters
            label: Service host
            include-workspace: false
            tooltip:
              Resource to host the service
          nports:
            type: number
            label: Number of Ports to Reserve
            hidden: true
            default: 1
            optional: true
          jobschedulertype:
            type: dropdown
            label: Select Controller, SLURM Partition or PBS Queue
            default: CONTROLLER
            options:
              - value: CONTROLLER
                label: Controller
              - value: SLURM
                label: SLURM Partition
              - value: PBS
                label: PBS Queue
            tooltip:
              Job will be submitted using SSH, sbatch or qsub, respectively
          _sch__dd_partition_e_:
            type: slurm-partitions
            label: SLURM partition
            hidden: ${{ 'SLURM' !== inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: true
            tooltip:
                Partition to submit the interactive job. Leave empty to let SLURM pick
                the optimal option.
            resource: ${{ inputs.pwrl_host.resource }}
          scheduler_directives_slurm:
            type: string
            label: Scheduler directives
            hidden: ${{ 'SLURM' !== inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: true
            tooltip:
                e.g. --mem=1000;--gpus-per-node=1 - Use the semicolon character ; to
                separate parameters. Do not include the SBATCH keyword.
          _sch__d_q___:
            type: string
            label: PBS queue
            hidden: ${{ 'PBS' !== inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: ${{ .hidden }}
            tooltip:
                Queue to submit the interactive job. Must select one! Use [qstat -f
                -Q] to list all queues on the system
          scheduler_directives_pbs:
            type: string
            label: Scheduler directives
            hidden: ${{ 'PBS' !== inputs.pwrl_host.jobschedulertype }}
            ignore: ${{ .hidden }}
            optional: true
            tooltip:
                e.g. -l mem=1000;-l nodes=1:ppn=4 - Use the semicolon character ; to
                separate parameters. Do not include the PBS keyword.
        collapsed: false
      service:
        type: group
        label: Service
        hidden: true
        items:
          name: 
            type: string
            hidden: true
            default: tailscale