<tool id='User.Demo_jupyter_docker_tensorflow_cloud' name='User.Demo_jupyter_docker_tensorflow_cloud'>
  <command interpreter='bash'>main.sh</command>
  <cancel interpreter='bash'>kill.sh</cancel>
  <inputs>
    <section name='service' type='section' title='Jupyter Server' expanded='true'>
        <param 
          name='name' 
          label='Service' 
          type='hidden' 
          value='r-singularity'  
        ></param>
        <param 
          name='path_to_sing' 
          label='Path to singularity container' 
          type='text' 
          value='/contrib/\${USER}/rserver.sif' 
          help='Path to the singularity container in the execution host'  
        ></param>
        <param 
          name="use_gpus" 
          type="boolean" 
          truevalue="Yes" 
          falsevalue="No" 
          checked="False" 
          label="Use GPUs?" 
          help='Select Yes to run a CUDA application inside a container'
        ></param>
    </section>
    <section name='pwrl_host' type='section' title='Service host' expanded='true'>
      <param 
        name='resource' 
        type='computeResource' 
        label='Service host' 
        hideUserWorkspace='true' 
        hideDisconnectedResources='false' 
        help='Resource to host the service'
      ></param>
      <param 
        name='nports' 
        label='Number of Ports to Reserve' 
        type='hidden' 
        value='1'  
      ></param> 
      <param 
        name='jobschedulertype' 
        type='select' 
        label='Select Controller or SLURM Partition' 
        help='Job will submitted using SSH or sbatch, respectively'      
        multiple='false'>
          <option value="CONTROLLER" selected="true">Controller</option>
          <option value="SLURM">SLURM Partition</option>
      </param>
      <param 
          name='_sch__dd_partition_e_' 
          label='SLURM partition' 
          type='dynamicPartitionDropdown' 
          resource='pwrl_host.resource'
          help='Partition to submit the interactive job. Leave empty to let SLURM pick the optimal option.' 
          depends_on='pwrl_host.jobschedulertype'
          show_if='SLURM'
          optional='true'
          dependent='false'   
      ></param>
      <param 
        name='scheduler_directives' 
        label='Scheduler directives' 
        type='text' 
        help='e.g. --mem=1000;--gpus-per-node=1 - Use the semicolon character ; to separate parameters. Do not include the SBATCH keyword.' 
        
        depends_on='pwrl_host.jobschedulertype'
        show_if='SLURM'
        optional='true'  
       ></param>
    </section>
  </inputs>
  <outputs>
  </outputs>
</tool>
